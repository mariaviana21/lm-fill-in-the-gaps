{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TBKUj8SQBEDo",
        "G8ixOEg_KOCc",
        "M5DZac7dMU-B",
        "7OKA0D07pGDU",
        "MyFA7ZohMiZk",
        "YWbO45QjM_SM",
        "B0XOZGpA_ca0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I am going to perform a fine-tuning of the bert-base-spanish-wwm-cased bert with a dataset of Tweets from the economic-political domain, and a fine-tuning of the distilbert-base-german-cased with a dataset of Amazon Reviews. The intention is to use this models combined with the idea presented by Chris Donahue et al. in their paper (https://arxiv.org/abs/2005.05339), which consists of a new approach to the task of infilling. Through the two models, which use [MASK] tokens to predict the words most likely to appear in those gaps, I use their main idea adapted to the possibilities of mine."
      ],
      "metadata": {
        "id": "8AlLTxLpBNnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Bert for spanish"
      ],
      "metadata": {
        "id": "TBKUj8SQBEDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, I download the bert-base-spanish-wwm-cased to see how the model performs and to know a little about its main features before proceeding to use it."
      ],
      "metadata": {
        "id": "ZFTqKxwuBuFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets transformers"
      ],
      "metadata": {
        "id": "9q_pw0K5CcSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)\n",
        "#I check the version of the transformers"
      ],
      "metadata": {
        "id": "NDqZEfxXCfCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Download the language model'''\n",
        "\n",
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "EpfaxEdwCiFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Check the parameters'''\n",
        "bert_num_parameters = model.num_parameters() / 1_000_000\n",
        "print(f\"'>>> BERT number of parameters: {round(bert_num_parameters)}M'\")"
      ],
      "metadata": {
        "id": "yjLAdGfSCn5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Create an example sentence for later'''\n",
        "text = \"Esto es un [MASK]\""
      ],
      "metadata": {
        "id": "wdmbKxIBCt_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Import a tokenizer. In this case, I'm using the model also as a tokenizer'''\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "pxRTohcyC0md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
      ],
      "metadata": {
        "id": "esDDx1PoDIca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I install the dataset, that can be found in the Hugging Face repository: https://huggingface.co/datasets/jhonparra18/petro-tweets"
      ],
      "metadata": {
        "id": "KXDdFKT7DQvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have not performed a cleaning of the dataset, but actually the model performance could be improved if the web links were removed, since there are quite a few Tweets and then appears as a probable word semicolons and commas."
      ],
      "metadata": {
        "id": "spSCiVUpEXRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rDSf3e4jDN9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('jhonparra18/petro-tweets','es')\n",
        "dataset"
      ],
      "metadata": {
        "id": "rIUibzQXDYRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''I create random samples, to check the data more accurately'''\n",
        "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['Tweet']}'\")"
      ],
      "metadata": {
        "id": "ZKFp4VJlDx31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Now I start with the Preprocessing of tha data. \n",
        "First of all, I tokenize the sentences.'''\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"Tweet\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "\n",
        "#I get rid of the columns, because I don't really need them for later\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=['Tweet','Date','User']\n",
        ")\n",
        "tokenized_datasets\n",
        "\n",
        "#The result should show input_ids, attention_mask, word_ids and in my case token_type_ids but not every Bert has this last one."
      ],
      "metadata": {
        "id": "hkao1NxlEQWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I check the models max_length to know more or less what chunk size to use\n",
        "tokenizer.model_max_length\n",
        "#I stick to this size because if not I will have problems with Colab\n",
        "chunk_size = 128"
      ],
      "metadata": {
        "id": "fYcrL4DmE5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing produces a list of lists for each feature\n",
        "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ],
      "metadata": {
        "id": "gyddMh-rFK_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ],
      "metadata": {
        "id": "X1AGVsojFoJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ],
      "metadata": {
        "id": "yIm8lbliFzgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "67tWTtuGF0Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ],
      "metadata": {
        "id": "_MR7bGq2F3g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "id": "jbj5lk7AF7Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
      ],
      "metadata": {
        "id": "eIZaKYtNGHYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "9cyK1kmZGJd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "cv-7kZZ9GT8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. \n",
        "\n",
        "This approach is called whole word masking. If I want to use whole word masking, I'll need to build a data collator.  "
      ],
      "metadata": {
        "id": "vRpxzH4mIns5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return default_data_collator(features)"
      ],
      "metadata": {
        "id": "rgtO33mvGYsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "AoKUTo1zIskX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first downsample the dataset before training, because if not it will take a lot of time and I will also run out of GPU."
      ],
      "metadata": {
        "id": "hgvBQJmQI0Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 5_000\n",
        "test_size = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ],
      "metadata": {
        "id": "IlH9xPPtI05d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''This is not really necessary if you don't want your model to be in Hugging Face'''\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lRR6tpXgI388"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training I have set the number of epochs to 20, just to make it quicker, but it does not give a really good result. And I also put remove_unused_columns to False, so that it does not mess up with the form of my data. \n",
        "\n",
        "Apart from this, the output_dir can be changed to your own computer."
      ],
      "metadata": {
        "id": "0AC1zWakJci8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Specify the arguments for the Trainer'''\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-tweets\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    push_to_hub=True,\n",
        "    logging_steps=logging_steps,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=20,\n",
        ")"
      ],
      "metadata": {
        "id": "SydlhZUEJKjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I call the trainer. I used the whole_word_masking_data_collator but it can also be used with a normal data_collator. "
      ],
      "metadata": {
        "id": "AHmiKc-IJkZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator= whole_word_masking_data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "NpnweM2_JV61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZNc5dp4mJugu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an evaluation I check the Perplexity. A lower perplexity score means a better language model, and we can see that the perplexity in this case is not that bad. "
      ],
      "metadata": {
        "id": "bL_YvnrOJtul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "kJPBlcvVJyHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "6XbjrJDGJ1qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\n",
        "    \"fill-mask\", model=\"mariav/bert-base-spanish-wwm-cased-finetuned-tweets\"\n",
        ")"
      ],
      "metadata": {
        "id": "gCChC86jJ32t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then feed the pipeline my sample text from before and see what the top 5 predictions are. It should show something similar to before, but also words included in the dataset used for fine-tuning. "
      ],
      "metadata": {
        "id": "skKGpGsUKCae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = mask_filler(text)\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\">>> {pred['sequence']}\")"
      ],
      "metadata": {
        "id": "zTIwgyQeKDFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning DistilBert for german"
      ],
      "metadata": {
        "id": "G8ixOEg_KOCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case I use the Distilbert-german-cased and the amazon-reviews-multilingual dataset.\n",
        "\n",
        "The process is the same as before, just changing what is necessary to include the new dataset."
      ],
      "metadata": {
        "id": "X1jcwYcIKUaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM\n",
        "\n",
        "model_checkpoint = \"distilbert-base-german-cased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "mbvEBan1LGiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
        "print(f\"'>>> Distilbert number of parameters: {round(distilbert_num_parameters)}M'\")"
      ],
      "metadata": {
        "id": "KGHgFrO-LJ4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Das ist eine gute [MASK]\""
      ],
      "metadata": {
        "id": "f3tWIq9ZLQVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "9zM9YyCFLSaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "token_logits = model(**inputs).logits\n",
        "# Find the location of [MASK] and extract its logits\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
      ],
      "metadata": {
        "id": "xczlwMQSLUV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('amazon_reviews_multi','de')\n",
        "dataset"
      ],
      "metadata": {
        "id": "Zibcor_BLWls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['review_body']}'\")"
      ],
      "metadata": {
        "id": "Slsrp6mTLZXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"review_body\"])\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "    return result\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"review_body\",'review_id','product_id','reviewer_id','stars','review_title','language','product_category']\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "vnF30-fcLc2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length\n",
        "\n",
        "chunk_size = 128"
      ],
      "metadata": {
        "id": "jgNqWUd-LhBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ],
      "metadata": {
        "id": "1Sfji9JULlfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ],
      "metadata": {
        "id": "T3wI4JFzLnsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ],
      "metadata": {
        "id": "3UCvjqsuLpo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "W35OL9lKLr8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ],
      "metadata": {
        "id": "uh1Iwj1JLt-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ],
      "metadata": {
        "id": "dh13LH3ALv9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
      ],
      "metadata": {
        "id": "kNrcYNc5Lxu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "O9DSx5jiLziK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "kE4fBpW5L2Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return default_data_collator(features)"
      ],
      "metadata": {
        "id": "tX0ll22wL33E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ],
      "metadata": {
        "id": "9GrAnxd2L7A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 9_000\n",
        "test_size = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ],
      "metadata": {
        "id": "w5v15vRHL9Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "nCX9LF4cL_Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-amazon-reviews\",\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    push_to_hub=True,\n",
        "    logging_steps=logging_steps,\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "G4FsnhKZMA7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator= whole_word_masking_data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "bz_4lc6PMDEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ePAOLSXvMFEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "OBY30xoaMHTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "BBiBgPT4MJjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\n",
        "    \"fill-mask\", model=\"mariav/distilbert-base-german-cased-finetuned-amazon-reviews\"\n",
        ")"
      ],
      "metadata": {
        "id": "aIkHZRrGMLT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = mask_filler(text)\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\">>> {pred['sequence']}\")"
      ],
      "metadata": {
        "id": "vACoZx9GMN0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ILM inference"
      ],
      "metadata": {
        "id": "M5DZac7dMU-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part what I do is adapt the main idea of the previously mentioned paper from Chris Donahue et al., because their ILM is not directly applicable to my own models. The basic idea behind it is to create my own fill-in-the-blank prompts using my own tokenizer and model. I create a context (such as 'La ___ es azul') related to the domain of my dataset, tokenize it using my own tokenizer, replace the blank (s) with special token(s) that my model recognizes as placeholders, and then pass the resulting tokens through my model to generate predictions for the missing word (s)."
      ],
      "metadata": {
        "id": "R2N2D1y-MYCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this I used two different approaches for both models:\n",
        "- Generating only one [MASK] in the sentence.\n",
        "- Generating two [MASK] in the sentence.\n",
        "\n",
        "For the first one I use the context, and for the second one I don't, just to prove the importance of giving a context, because it really changes the results."
      ],
      "metadata": {
        "id": "BDQcCLOAlgdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spanish-BERT"
      ],
      "metadata": {
        "id": "7OKA0D07pGDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generating one [MASK]\n",
        "For the generation of only one [MASK] in the sentence I use a pre-trained tokenizer and language model to generate predictions for masked tokens in a list of sentences. \n",
        "\n",
        "I load the tokenizer and model using AutoTokenizer and AutoModelForMaskedLM from the Transformers library. I define a list of sentences with masked tokens and a context sentence, and tokenize both using the tokenizer. The context sentence is repeated for each input sentence and the resulting tensors are concatenated along the sequence dimension.\n",
        "\n",
        "Finally, the model is used to generate predictions for the masked tokens in the concatenated tensor, and the predicted tokens are decoded and printed for each input sentence. "
      ],
      "metadata": {
        "id": "MyFA7ZohMiZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mariav/bert-base-spanish-wwm-cased-finetuned-tweets\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"mariav/bert-base-spanish-wwm-cased-finetuned-tweets\")\n",
        "\n",
        "# Define the sentences with masked tokens and context\n",
        "sentences = [\n",
        "    \"Hoy en día, [MASK] es un tema muy importante en la sociedad.\",\n",
        "    \"La [MASK] es una de las preocupaciones más importantes en la política actual.\",\n",
        "    \"La [MASK] en la educación es un problema que se debe abordar.\",\n",
        "    \"La [MASK] de género es un tema que necesita más atención en nuestra sociedad.\",\n",
        "]\n",
        "\n",
        "context = \"La corrupción es uno de los principales problemas en la política y la sociedad actual.\"\n",
        "\n",
        "# Tokenize the sentences \n",
        "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize the context\n",
        "tokenized_context = tokenizer(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Repeat the tokenized context for each input sentence\n",
        "num_sentences = len(sentences)\n",
        "repeated_context = {}\n",
        "for k, v in tokenized_context.items():\n",
        "    repeated_context[k] = v.repeat(num_sentences, 1)\n",
        "# Tokenize the sentences and prepend the context\n",
        "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "tokenized_sentences[\"input_ids\"] = torch.cat([repeated_context[\"input_ids\"], tokenized_sentences[\"input_ids\"]], dim=1)\n",
        "tokenized_sentences[\"attention_mask\"] = torch.cat([repeated_context[\"attention_mask\"], tokenized_sentences[\"attention_mask\"]], dim=1)\n",
        "\n",
        "# Generate predictions for the masked tokens in the sentences\n",
        "with torch.no_grad():\n",
        "    outputs = model(torch.tensor(tokenized_sentences[\"input_ids\"]), attention_mask=torch.tensor(tokenized_sentences[\"attention_mask\"]))\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "# Print the predicted tokens\n",
        "for i, sentence in enumerate(sentences):\n",
        "    mask_index = torch.where(tokenized_sentences[\"input_ids\"][i] == tokenizer.mask_token_id)[0][0]\n",
        "    token = predictions[i][mask_index].item()\n",
        "    predicted_token = tokenizer.decode(token)\n",
        "    completed_sentence = sentence.replace('[MASK]', predicted_token)\n",
        "    print(completed_sentence)\n"
      ],
      "metadata": {
        "id": "v1iWUhkrMerS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using two [MASK]\n",
        "For this part, I create a function (predict_missing_words) that two words to be predicted in the same sentence. I consider this one is not really great, taking into account that is not done randomly, but you need to specify which words of the sentence you want to [MASK]. "
      ],
      "metadata": {
        "id": "mxLwqshboLQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mariav/bert-base-spanish-wwm-cased-finetuned-tweets\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"mariav/bert-base-spanish-wwm-cased-finetuned-tweets\")\n",
        "\n",
        "def predict_missing_words(sentence, mask_words=[\"\", \"\"], top_k=5):\n",
        "    # Encode the sentence with special tokens and get the token IDs\n",
        "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "    token_ids = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Find indices of words to mask\n",
        "    mask_idx = []\n",
        "    for mask_word in mask_words:\n",
        "        mask_idx += [i for i, tok_id in enumerate(token_ids) if tok_id == mask_word]\n",
        "\n",
        "    # Mask words and get new input IDs\n",
        "    masked_input_ids = input_ids.copy()\n",
        "    for i in mask_idx:\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "    # Convert input IDs to tensors\n",
        "    input_ids_tensor = torch.tensor([input_ids])\n",
        "    masked_input_ids_tensor = torch.tensor([masked_input_ids])\n",
        "\n",
        "    # Generate predictions for the masked tokens\n",
        "    with torch.no_grad():\n",
        "        predictions = model(masked_input_ids_tensor)[0]\n",
        "\n",
        "    # Get top-k predicted words for each masked token\n",
        "    predicted_words = []\n",
        "    for i in mask_idx:\n",
        "        predicted_token_ids = predictions[0, i].topk(k=top_k).indices.tolist()\n",
        "        predicted_words.append([tokenizer.convert_ids_to_tokens([tok_id])[0] for tok_id in predicted_token_ids])\n",
        "\n",
        "    # Generate all possible sentence combinations\n",
        "    sentence_combinations = [input_ids]\n",
        "    for i, predicted_word_set in enumerate(predicted_words):\n",
        "        new_sentence_combinations = []\n",
        "        for sentence in sentence_combinations:\n",
        "            for predicted_word in predicted_word_set:\n",
        "                new_sentence = sentence.copy()\n",
        "                new_sentence[mask_idx[i]] = tokenizer.convert_tokens_to_ids(predicted_word)\n",
        "                new_sentence_combinations.append(new_sentence)\n",
        "        sentence_combinations = new_sentence_combinations\n",
        "\n",
        "    # Convert all sentence combinations to strings and return them\n",
        "    return [tokenizer.decode(sentence) for sentence in sentence_combinations]"
      ],
      "metadata": {
        "id": "AwVBzJ-ZM3vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sentence = \"La república es una falsa democracia.\"\n",
        "predicted_sentences = predict_missing_words(sentence, mask_words=[\"república\", \"democracia\"], top_k=5)\n",
        "for predicted_sentence in predicted_sentences:\n",
        "  print(predicted_sentence)"
      ],
      "metadata": {
        "id": "XIQ_PoI6M8gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## German-DistilBert"
      ],
      "metadata": {
        "id": "YWbO45QjM_SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part, I do the same two approaches and I check how it works for the german model, just adding different sentences and a context more adapted to the dataset."
      ],
      "metadata": {
        "id": "dqv13ty0pLAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating only one [MASK]"
      ],
      "metadata": {
        "id": "B0XOZGpA_ca0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mariav/distilbert-base-german-cased-finetuned-amazon-reviews\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"mariav/distilbert-base-german-cased-finetuned-amazon-reviews\")\n",
        "\n",
        "# Define the sentences \n",
        "sentences = [\"Das [MASK] sagt mir nicht zu.\", 'Ich empfehle [MASK] allen.','Ich lese immer [MASK].','Die [MASK] haben mir geholfen.']\n",
        "context = \"Ich habe kürzlich ein Produkt auf Amazon gekauft und war mit der Qualität und dem Service sehr zufrieden.\"\n",
        "\n",
        "# Tokenize the sentences with the provided context\n",
        "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize the context\n",
        "tokenized_context = tokenizer(context, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Repeat the tokenized context for each input sentence\n",
        "num_sentences = len(sentences)\n",
        "repeated_context = {}\n",
        "for k, v in tokenized_context.items():\n",
        "    repeated_context[k] = v.repeat(num_sentences, 1)\n",
        "\n",
        "# Tokenize the sentences and prepend the context\n",
        "tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "tokenized_sentences[\"input_ids\"] = torch.cat([repeated_context[\"input_ids\"], tokenized_sentences[\"input_ids\"]], dim=1)\n",
        "tokenized_sentences[\"attention_mask\"] = torch.cat([repeated_context[\"attention_mask\"], tokenized_sentences[\"attention_mask\"]], dim=1)\n",
        "\n",
        "# Generate predictions for the masked tokens in the sentences\n",
        "with torch.no_grad():\n",
        "    outputs = model(torch.tensor(tokenized_sentences[\"input_ids\"]), attention_mask=torch.tensor(tokenized_sentences[\"attention_mask\"]))\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "# Print the predicted tokens\n",
        "for i, sentence in enumerate(sentences):\n",
        "    mask_index = torch.where(tokenized_sentences[\"input_ids\"][i] == tokenizer.mask_token_id)[0][0]\n",
        "    token = predictions[i][mask_index].item()\n",
        "    predicted_token = tokenizer.decode(token)\n",
        "    completed_sentence = sentence.replace('[MASK]', predicted_token)\n",
        "    print(completed_sentence)"
      ],
      "metadata": {
        "id": "nCaScxLdNBs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating two [MASK]"
      ],
      "metadata": {
        "id": "ZTDdFYub_lOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mariav/distilbert-base-german-cased-finetuned-amazon-reviews\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"mariav/distilbert-base-german-cased-finetuned-amazon-reviews\")\n",
        "\n",
        "def predict_missing_words(sentence, mask_words=[\"\", \"\"], top_k=5):\n",
        "    # Encode the sentence with special tokens and get the token IDs\n",
        "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "    token_ids = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Find indices of words to mask\n",
        "    mask_idx = []\n",
        "    for mask_word in mask_words:\n",
        "        mask_idx += [i for i, tok_id in enumerate(token_ids) if tok_id == mask_word]\n",
        "\n",
        "    # Mask words and get new input IDs\n",
        "    masked_input_ids = input_ids.copy()\n",
        "    for i in mask_idx:\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "    # Convert input IDs to tensors\n",
        "    input_ids_tensor = torch.tensor([input_ids])\n",
        "    masked_input_ids_tensor = torch.tensor([masked_input_ids])\n",
        "\n",
        "    # Generate predictions for the masked tokens\n",
        "    with torch.no_grad():\n",
        "        predictions = model(masked_input_ids_tensor)[0]\n",
        "\n",
        "    # Get top-k predicted words for each masked token\n",
        "    predicted_words = []\n",
        "    for i in mask_idx:\n",
        "        predicted_token_ids = predictions[0, i].topk(k=top_k).indices.tolist()\n",
        "        predicted_words.append([tokenizer.convert_ids_to_tokens([tok_id])[0] for tok_id in predicted_token_ids])\n",
        "\n",
        "    # Generate all possible sentence combinations\n",
        "    sentence_combinations = [input_ids]\n",
        "    for i, predicted_word_set in enumerate(predicted_words):\n",
        "        new_sentence_combinations = []\n",
        "        for sentence in sentence_combinations:\n",
        "            for predicted_word in predicted_word_set:\n",
        "                new_sentence = sentence.copy()\n",
        "                new_sentence[mask_idx[i]] = tokenizer.convert_tokens_to_ids(predicted_word)\n",
        "                new_sentence_combinations.append(new_sentence)\n",
        "        sentence_combinations = new_sentence_combinations\n",
        "\n",
        "    # Convert all sentence combinations to strings and return them\n",
        "    return [tokenizer.decode(sentence) for sentence in sentence_combinations]\n"
      ],
      "metadata": {
        "id": "7Vm8r5TONEzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sentence = \"Das Produkt hat mir nicht gefallen.\"\n",
        "predicted_sentences = predict_missing_words(sentence, mask_words=[\"Produkt\", \"gefallen\"], top_k=5)\n",
        "print(predicted_sentences)"
      ],
      "metadata": {
        "id": "fX0N6hsHNIgw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}